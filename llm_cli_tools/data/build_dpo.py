import json
import os
from collections import defaultdict, Counter
import argparse
from pathlib import Path
from llm_cli_tools.utils.file_utils import load_json, load_jsonl, save_json, save_jsonl


def load_data(filepath):
    """è‡ªåŠ¨åˆ¤æ–­æ–‡ä»¶ç±»åž‹å¹¶åŠ è½½æ•°æ®"""
    if filepath.endswith('.jsonl'):
        return load_jsonl(filepath)
    elif filepath.endswith('.json'):
        return load_json(filepath)
    else:
        print(f"âš ï¸ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {filepath}")
        return []


def analyze_scores_detailed(data, verbose=False):
    """è¯¦ç»†åˆ†æ•°åˆ†æž"""
    scores = [x.get('score', 0) for x in data if 'score' in x]
    
    if not scores:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°åˆ†æ•°æ•°æ®")
        return
    
    print(f"\n=== æ•°æ®æ¦‚è§ˆ ===")
    print(f"æ€»æ ·æœ¬æ•°: {len(data)}")
    print(f"æœ‰æ•ˆåˆ†æ•°æ ·æœ¬æ•°: {len(scores)}")
    print(f"å¹³å‡åˆ†: {sum(scores)/len(scores):.2f}")
    print(f"æœ€é«˜åˆ†: {max(scores):.2f}")
    print(f"æœ€ä½Žåˆ†: {min(scores):.2f}")
    
    if verbose:
        print(f"\n=== åˆ†æ•°åˆ†å¸ƒ ===")
        score_counter = Counter(scores)
        for score in sorted(score_counter.keys(), reverse=True):
            print(f"åˆ†æ•° {score}: {score_counter[score]} ä¸ªæ ·æœ¬")


def build_ref_map(ref_data, id_key='id', round_key='round'):
    """
    æž„å»ºå‚è€ƒæ–‡ä»¶ç´¢å¼•: (id, round) -> {messages, output}
    
    Args:
        ref_data: å‚è€ƒæ•°æ®åˆ—è¡¨
        id_key: ID å­—æ®µå
        round_key: è½®æ¬¡å­—æ®µå
    
    Returns:
        ref_map: ç´¢å¼•å­—å…¸
    """
    print(f"æ­£åœ¨æž„å»ºå‚è€ƒæ–‡ä»¶ç´¢å¼• (æºæ–‡ä»¶: {len(ref_data)} æ¡)...")
    ref_map = {}
    duplicates = 0
    
    for item in ref_data:
        key = (item.get(id_key), item.get(round_key))
        if key in ref_map:
            duplicates += 1
            continue
        
        ref_map[key] = {
            "messages": item.get('messages', []),
            "output": item.get('output', '')
        }
    
    if duplicates > 0:
        print(f"  è­¦å‘Š: å‘çŽ° {duplicates} ä¸ªé‡å¤çš„ï¼Œå·²è·³è¿‡")
    print(f"  ç´¢å¼•æž„å»ºå®Œæˆ: {len(ref_map)} ä¸ªå”¯ä¸€é”®\n")
    return ref_map


def get_prompt_from_messages(messages):
    """
    ä»Ž messages ä¸­æå– promptã€‚
    é€»è¾‘ï¼šæå– system å’Œ user çš„å†…å®¹
    """
    if not messages:
        return '', ''
    
    instruction = ''
    input_ = ''
    for item in messages:
        if item.get('role') == 'system':
            instruction = item.get('content', '')
        if item.get('role') == 'user':
            input_ = item.get('content', '')

    return instruction, input_


def build_dpo_dataset(score_data, ref_map, min_margin, min_chosen_score, 
                      id_key='id', round_key='round', verbose=False):
    """
    æž„å»ºå®Œæ•´çš„ DPO æ•°æ®é›† (åŒ…å«æ–‡æœ¬å†…å®¹)
    
    Args:
        score_data: å¸¦åˆ†æ•°çš„æ•°æ®åˆ—è¡¨
        ref_map: å‚è€ƒæ•°æ®ç´¢å¼•
        min_margin: æœ€å°åˆ†å·®
        min_chosen_score: æ­£æ ·æœ¬æœ€ä½Žåˆ†
        id_key: ID å­—æ®µå
        round_key: è½®æ¬¡å­—æ®µå
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    
    Returns:
        dpo_list: DPO æ•°æ®åˆ—è¡¨
        filtered_list: è¢«è¿‡æ»¤çš„æ ·æœ¬åˆ—è¡¨
    """
    print(f"å¼€å§‹æž„å»º DPO æ•°æ® (Margin > {min_margin}, Chosen >= {min_chosen_score})...")
    
    # 1. æŒ‰ ID åˆ†ç»„
    groups = defaultdict(list)
    for item in score_data:
        id_val = item.get(id_key)
        if id_val is not None:
            groups[id_val].append(item)
    
    dpo_list = []
    filtered_list = []
    
    stats = {
        "total_groups": len(groups),
        "valid_pairs": 0,
        "filtered_small_margin": 0,
        "filtered_bad_chosen": 0,
        "filtered_missing_ref": 0,
        "filtered_single_item": 0
    }
    
    for id_val, items in groups.items():
        if len(items) < 2:
            stats["filtered_single_item"] += 1
            if verbose:
                filtered_list.append({
                    "id": id_val,
                    "reason": "Only one sample in group",
                    "count": len(items)
                })
            continue
        
        # æŒ‰åˆ†æ•°æŽ’åº
        items_sorted = sorted(items, key=lambda x: x.get('score', 0), reverse=True)
        
        chosen_meta = items_sorted[0]
        rejected_meta = items_sorted[-1]
        
        chosen_score = chosen_meta.get('score', 0)
        rejected_score = rejected_meta.get('score', 0)
        score_diff = chosen_score - rejected_score
        
        # --- è¿‡æ»¤é€»è¾‘ ---
        reason = None
        
        # 1. æ£€æŸ¥åˆ†å·®
        if score_diff < min_margin:
            stats["filtered_small_margin"] += 1
            reason = f"Margin too small ({score_diff} < {min_margin})"
        
        # 2. æ£€æŸ¥æ­£æ ·æœ¬è´¨é‡
        elif chosen_score < min_chosen_score:
            stats["filtered_bad_chosen"] += 1
            reason = f"Chosen score too low ({chosen_score} < {min_chosen_score})"
        
        # 3. æ£€æŸ¥å‚è€ƒæ–‡ä»¶ä¸­æ˜¯å¦å­˜åœ¨å¯¹åº”çš„æ–‡æœ¬æ•°æ®
        else:
            chosen_key = (id_val, chosen_meta.get(round_key))
            rejected_key = (id_val, rejected_meta.get(round_key))
            if chosen_key not in ref_map or rejected_key not in ref_map:
                stats["filtered_missing_ref"] += 1
                reason = "Missing reference text (output/messages)"
        
        if reason:
            # è®°å½•è¢«è¿‡æ»¤çš„æ ·æœ¬ä»¥ä¾¿è°ƒè¯•
            filtered_list.append({
                "id": id_val,
                "reason": reason,
                "chosen_score": chosen_score,
                "rejected_score": rejected_score,
                "rounds": [chosen_meta.get(round_key), rejected_meta.get(round_key)]
            })
            continue

        # --- æž„å»ºæœ‰æ•ˆ DPO å¯¹ ---
        chosen_ref = ref_map[chosen_key]
        rejected_ref = ref_map[rejected_key]
        
        instruction, input_ = get_prompt_from_messages(chosen_ref['messages'])
        
        dpo_item = {
            "instruction": instruction, 
            "input": input_,
            "chosen": chosen_ref['output'],
            "rejected": rejected_ref['output'],
            "id": id_val,
            "chosen_round": chosen_meta.get(round_key),
            "rejected_round": rejected_meta.get(round_key),
            "chosen_score": chosen_score,
            "rejected_score": rejected_score,
            "margin": score_diff
        }
        dpo_list.append(dpo_item)
        stats["valid_pairs"] += 1

    # æ‰“å°ç»Ÿè®¡
    print(f"\n=== æž„å»ºç»“æžœ ===")
    print(f"æ€» ID ç»„æ•°: {stats['total_groups']}")
    print(f"âœ… æœ‰æ•ˆ DPO å¯¹: {stats['valid_pairs']}")
    print(f"âŒ è¿‡æ»¤æ ·æœ¬æ€»æ•°: {len(filtered_list)}")
    print(f"   - å•æ ·æœ¬ç»„: {stats['filtered_single_item']}")
    print(f"   - åˆ†å·®ä¸è¶³: {stats['filtered_small_margin']}")
    print(f"   - æ­£æ ·æœ¬åˆ†ä½Ž: {stats['filtered_bad_chosen']}")
    print(f"   - ç¼ºå¤±æ–‡æœ¬æ•°æ®: {stats['filtered_missing_ref']}")
    
    return dpo_list, filtered_list


def save_data(filepath, data):
    """æ ¹æ®æ–‡ä»¶æ‰©å±•åè‡ªåŠ¨é€‰æ‹©ä¿å­˜æ ¼å¼"""
    if filepath.endswith('.jsonl'):
        save_jsonl(data, filepath)
    elif filepath.endswith('.json'):
        save_json(data, filepath)
    else:
        print(f"âš ï¸ ä¸æ”¯æŒçš„è¾“å‡ºæ ¼å¼: {filepath}")


def main():
    parser = argparse.ArgumentParser(
        description="æž„å»º DPO (Direct Preference Optimization) æ•°æ®é›†",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # åŸºæœ¬ç”¨æ³•
  python build_dpo.py score_data.jsonl ref_data.jsonl -o dpo_output.jsonl
  
  # è‡ªå®šä¹‰é˜ˆå€¼
  python build_dpo.py score_data.jsonl ref_data.jsonl -o dpo_output.jsonl --min-margin 15 --min-chosen-score 70
  
  # ä¿å­˜è¿‡æ»¤æ—¥å¿—
  python build_dpo.py score_data.jsonl ref_data.jsonl -o dpo_output.jsonl --save-filtered filtered_log.jsonl
  
  # ä½¿ç”¨ JSON æ ¼å¼è¾“å…¥
  python build_dpo.py score_data.json ref_data.json -o dpo_output.json
  
  # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
  python build_dpo.py score_data.jsonl ref_data.jsonl -o dpo_output.jsonl --verbose

æ•°æ®æ ¼å¼è¯´æ˜Ž:
  åˆ†æ•°æ–‡ä»¶æ ¼å¼:
    {
      "id": "sample_001",
      "round": 1,
      "score": 85.5
    }
  
  å‚è€ƒæ–‡ä»¶æ ¼å¼:
    {
      "id": "sample_001",
      "round": 1,
      "messages": [
        {"role": "system", "content": "ç³»ç»Ÿæç¤º"},
        {"role": "user", "content": "ç”¨æˆ·è¾“å…¥"}
      ],
      "output": "æ¨¡åž‹è¾“å‡º"
    }
  
  è¾“å‡ºæ–‡ä»¶æ ¼å¼:
    {
      "instruction": "ç³»ç»Ÿæç¤º",
      "input": "ç”¨æˆ·è¾“å…¥",
      "chosen": "ä¼˜é€‰è¾“å‡º",
      "rejected": "æ‹’ç»è¾“å‡º",
      "id": "sample_001",
      "chosen_round": 1,
      "rejected_round": 2,
      "chosen_score": 85.5,
      "rejected_score": 45.2,
      "margin": 40.3
    }
        """
    )
    
    parser.add_argument(
        'score_file',
        help='åˆ†æ•°æ–‡ä»¶è·¯å¾„ï¼ˆJSON æˆ– JSONL æ ¼å¼ï¼‰'
    )
    
    parser.add_argument(
        'ref_file',
        help='å‚è€ƒæ–‡ä»¶è·¯å¾„ï¼ˆJSON æˆ– JSONL æ ¼å¼ï¼ŒåŒ…å« messages å’Œ outputï¼‰'
    )
    
    parser.add_argument(
        '-o', '--output',
        required=True,
        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆæ ¹æ®æ‰©å±•åè‡ªåŠ¨é€‰æ‹© JSON æˆ– JSONL æ ¼å¼ï¼‰'
    )
    
    parser.add_argument(
        '--min-margin',
        type=float,
        default=20.0,
        help='æœ€å°åˆ†å·®é˜ˆå€¼ï¼ˆé»˜è®¤: 20.0ï¼‰'
    )
    
    parser.add_argument(
        '--min-chosen-score',
        type=float,
        default=60.0,
        help='æ­£æ ·æœ¬æœ€ä½Žåˆ†æ•°é˜ˆå€¼ï¼ˆé»˜è®¤: 60.0ï¼‰'
    )
    
    parser.add_argument(
        '--save-filtered',
        type=str,
        metavar='FILE',
        help='ä¿å­˜è¢«è¿‡æ»¤çš„æ ·æœ¬æ—¥å¿—åˆ°æŒ‡å®šæ–‡ä»¶'
    )
    
    parser.add_argument(
        '--id-key',
        type=str,
        default='id',
        help='ID å­—æ®µåï¼ˆé»˜è®¤: idï¼‰'
    )
    
    parser.add_argument(
        '--round-key',
        type=str,
        default='round',
        help='è½®æ¬¡å­—æ®µåï¼ˆé»˜è®¤: roundï¼‰'
    )
    
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯'
    )
    
    args = parser.parse_args()
    
    # 1. åŠ è½½å¸¦åˆ†æ•°çš„æ•°æ®
    print("Step 1: åŠ è½½åˆ†æ•°æ•°æ®...")
    score_data = load_data(args.score_file)
    if not score_data:
        print("âŒ é”™è¯¯: åˆ†æ•°æ•°æ®ä¸ºç©º")
        return
        
    analyze_scores_detailed(score_data, verbose=args.verbose)
    
    # 2. åŠ è½½å‚è€ƒæ–‡æœ¬æ•°æ® (ç”¨äºŽèŽ·å– output å’Œ messages)
    print("\nStep 2: åŠ è½½å‚è€ƒæ–‡æœ¬æ•°æ®...")
    ref_data = load_data(args.ref_file)
    if not ref_data:
        print("âŒ é”™è¯¯: å‚è€ƒæ–‡æœ¬æ•°æ®ä¸ºç©º")
        return
        
    ref_map = build_ref_map(ref_data, id_key=args.id_key, round_key=args.round_key)
    
    # 3. æž„å»º DPO æ•°æ®
    print("\nStep 3: æž„å»º DPO æ•°æ®é›†...")
    dpo_list, filtered_list = build_dpo_dataset(
        score_data, ref_map, 
        min_margin=args.min_margin, 
        min_chosen_score=args.min_chosen_score,
        id_key=args.id_key, 
        round_key=args.round_key,
        verbose=args.verbose
    )
    
    # 4. ä¿å­˜ç»“æžœ
    print("\nStep 4: ä¿å­˜ç»“æžœ...")
    
    if dpo_list:
        save_data(args.output, dpo_list)
    else:
        print("âš ï¸ æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆçš„ DPO æ•°æ®å¯¹")
    
    if args.save_filtered and filtered_list:
        save_data(args.save_filtered, filtered_list)
        print(f"\nðŸ“ è¢«è¿‡æ»¤çš„æ ·æœ¬æ—¥å¿—å·²ä¿å­˜è‡³: {args.save_filtered}")
        print("   ä½ å¯ä»¥æŸ¥çœ‹æ­¤æ–‡ä»¶è°ƒæ•´é˜ˆå€¼")

    print("\nðŸŽ‰ å¤„ç†å®Œæˆ!")


if __name__ == "__main__":
    main()
